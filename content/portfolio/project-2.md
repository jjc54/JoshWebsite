---
categories:
- Statistics
- Deep Learning
date: "2024-03-20T12:14:34+06:00"
description: CLIP Chronicles - Bridging Pixels and Prose in Deep Learning
draft: false
github_url: https://github.com/jjc54/IDC6146_Project_CLIP_Chronicles
image: images/portfolio/CLIP.png
project_url: '[View Project](https://jjc54.github.io/IDC6146_Project_CLIP_Chronicles/)'
title: CLIP Chronicles - Bridging Pixels and Prose in Deep Learning
---


#### Project Details

Contrastive Language-Image Pre-training (CLIP) is a Convolutional Neural Network (CNN) that learns visual concepts from natural language supervision and has been trained on a combination of images and their captions. CLIP bridges the gap between traditional CNN and natural language processing, providing several innovations and advancements such as generalization across tasks, increased flexibility, and scalability, and having an interface to natural language.

As part of Deep Learning for Data Science (IDC 6146) at the University of West Florida, our team critically analyzed, assessed, and reproduced the CLIP model and elaborated on potential applications in a professional report. This website was built using R/Quarto while the code for the analysis was built using Python/Jupyter, representing an interdisciplinary and collaborative approach to this group project.

This project was completed under the guidance of Dr. Shusen Pu (https://www.shusenpu.com/)

#### Project Highlights

✅ Academic Group Project - Part of UWF M.S. Data Science (https://uwf.edu/programs/hmcse/data-science-ms/)

✅ Comprehensive Literature Review on CLIP and CNNs

✅ Full By-Example CLIP training/testing/implementation with Python; Report written in R

✅ Final Report, Code, and Presentation Publicly Available